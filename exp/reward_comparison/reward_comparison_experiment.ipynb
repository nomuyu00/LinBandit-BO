{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 報酬設計の比較実験\n",
    "\n",
    "このノートブックでは、Continuous ECI-BO-Banditアルゴリズムについて、2つの異なる報酬設計を比較します：\n",
    "\n",
    "1. **元の報酬設計**: 予測誤差に基づく報酬計算\n",
    "2. **新しい報酬設計**: GPモデルの勾配の絶対値を使った報酬計算\n",
    "\n",
    "## 実験設定\n",
    "- テスト関数: Styblinski-Tang, Rastrigin, Ackley (100次元中先頭5次元が有効)\n",
    "- 20回の独立実行\n",
    "- 300回の評価\n",
    "- 収束性能と方向選択の比較"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:47.739560Z",
     "start_time": "2025-07-11T04:26:41.640264Z"
    }
   },
   "source": "import math\nimport os\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n# BoTorch / GPyTorch\nfrom botorch import fit_gpytorch_model\nfrom botorch.models import SingleTaskGP\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom botorch.acquisition import ExpectedImprovement\nfrom botorch.optim import optimize_acqf\n\n# デフォルトのdtypeをfloat32に設定\ntorch.set_default_dtype(torch.float32)\n\n# プロット設定\nplt.rcParams[\"figure.dpi\"] = 100\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomuyu\\PycharmProjects\\LinBandit-BO\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト関数群（100次元中先頭5次元が有効）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:48.023858Z",
     "start_time": "2025-07-11T04:26:48.009856Z"
    }
   },
   "source": [
    "def styblinski_tang_100d(x, noise_std=1e-5):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    x5 = x[..., :5]\n",
    "    res = 0.5 * torch.sum(x5**4 - 16.0*x5**2 + 5.0*x5, dim=-1)\n",
    "    return res + torch.randn_like(res) * noise_std\n",
    "\n",
    "def rastrigin_100d(x, noise_std=1e-5):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    x5 = x[..., :5]\n",
    "    s = torch.sum(x5**2 - 10.0*torch.cos(2*math.pi*x5) + 10.0, dim=-1)\n",
    "    return s + torch.randn_like(s) * noise_std\n",
    "\n",
    "def ackley_100d(x, noise_std=1e-5):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    x5 = x[..., :5]\n",
    "    d = 5\n",
    "    sum_sq = torch.sum(x5**2, dim=-1)\n",
    "    r = torch.sqrt(sum_sq / d)\n",
    "    part1 = -20.0 * torch.exp(-0.2 * r)\n",
    "    part2 = -torch.exp(torch.mean(torch.cos(2.0*math.pi*x5), dim=-1))\n",
    "    res = part1 + part2 + 20.0 + math.e\n",
    "    return res + torch.randn_like(res) * noise_std"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベースクラス: ECI_BO_Bandit_Original"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:48.055858Z",
     "start_time": "2025-07-11T04:26:48.040856Z"
    }
   },
   "source": [
    "class ECI_BO_Bandit_Original:\n",
    "    def __init__(self, X, objective_function, bounds, n_initial, n_max, dim,\n",
    "                 algo_base_name=\"ECI_BO_Bandit_Original\", coordinate_ratio=0.8, run_id=1, output_base_dir=\"output_results\"):\n",
    "        self.X = X.float()\n",
    "        self.dim = dim\n",
    "        self.num_arms = dim\n",
    "        self.A = torch.eye(dim)\n",
    "        self.b = torch.zeros(dim)\n",
    "        self.objective_function = objective_function\n",
    "        self.bounds = bounds.float()\n",
    "        self.n_initial = n_initial\n",
    "        self.n_max = n_max\n",
    "        self.Y = None\n",
    "        self.best_value = None\n",
    "        self.best_point = None\n",
    "        self.model = None\n",
    "        self.eval_history = []\n",
    "        self.selected_direction_history = []\n",
    "        self.theta_history = []\n",
    "        self.coordinate_ratio = coordinate_ratio\n",
    "        self.scale_init = 1.0\n",
    "        self.run_id = run_id\n",
    "\n",
    "        self.function_name_with_ratio = f\"Original_coord_{self.coordinate_ratio:.1f}\"\n",
    "        self.algo_name_for_run = f\"{algo_base_name}_{self.function_name_with_ratio}_run{self.run_id}\"\n",
    "\n",
    "        self.output_dir = os.path.join(output_base_dir, algo_base_name, self.function_name_with_ratio)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.total_iterations_for_bandit = 0\n",
    "\n",
    "    def update_model(self):\n",
    "        kernel = ScaleKernel(\n",
    "            RBFKernel(ard_num_dims=self.X.shape[-1], dtype=torch.float32),\n",
    "            dtype=torch.float32, noise_constraint=1e-3\n",
    "        ).to(self.X)\n",
    "        self.model = SingleTaskGP(self.X, self.Y, covar_module=kernel)\n",
    "        mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "    def initialize(self):\n",
    "        y_val = self.objective_function(self.X)\n",
    "        self.Y = y_val.unsqueeze(-1).float()\n",
    "        y_max, y_min = self.Y.max().item(), self.Y.min().item()\n",
    "        self.scale_init = (y_max - y_min) if (y_max - y_min) != 0 else 1.0\n",
    "        self.update_model()\n",
    "        post_mean = self.model.posterior(self.X).mean.squeeze(-1)\n",
    "        bi = post_mean.argmin()\n",
    "        self.best_value = post_mean[bi].item()\n",
    "        self.best_point = self.X[bi]\n",
    "        self.eval_history = [self.best_value] * self.n_initial\n",
    "\n",
    "    def propose_new_x(self, direction):\n",
    "        ei = ExpectedImprovement(self.model, best_f=self.best_value, maximize=False)\n",
    "        \n",
    "        active_dims_mask = direction.abs() > 1e-9\n",
    "        if not active_dims_mask.any():\n",
    "            lb, ub = -1.0, 1.0\n",
    "        else:\n",
    "            ratios_lower = (self.bounds[0] - self.best_point) / (direction + 1e-12 * (~active_dims_mask))\n",
    "            ratios_upper = (self.bounds[1] - self.best_point) / (direction + 1e-12 * (~active_dims_mask))\n",
    "\n",
    "            t_bounds = torch.zeros(self.dim, 2, device=self.X.device)\n",
    "            t_bounds[:, 0] = torch.minimum(ratios_lower, ratios_upper)\n",
    "            t_bounds[:, 1] = torch.maximum(ratios_lower, ratios_upper)\n",
    "\n",
    "            lb = -float('inf')\n",
    "            ub = float('inf')\n",
    "            for i in range(self.dim):\n",
    "                if active_dims_mask[i]:\n",
    "                    lb = max(lb, t_bounds[i,0].item())\n",
    "                    ub = min(ub, t_bounds[i,1].item())\n",
    "\n",
    "        if lb > ub:\n",
    "            lb, ub = -1.0, 1.0\n",
    "            if self.best_point is not None:\n",
    "                domain_width = (self.bounds[1,0] - self.bounds[0,0]).item()\n",
    "                lb = -0.1 * domain_width\n",
    "                ub =  0.1 * domain_width\n",
    "\n",
    "        one_d_bounds = torch.tensor([[lb],[ub]], dtype=torch.float32, device=self.X.device)\n",
    "\n",
    "        def ei_on_line(t_scalar_tensor):\n",
    "            t_values = t_scalar_tensor.squeeze(-1)\n",
    "            points_on_line = self.best_point.unsqueeze(0) + t_values.reshape(-1,1) * direction.unsqueeze(0)\n",
    "            points_on_line_clamped = torch.clamp(points_on_line, self.bounds[0].unsqueeze(0), self.bounds[1].unsqueeze(0))\n",
    "            return ei(points_on_line_clamped.unsqueeze(1))\n",
    "\n",
    "        cand_t, acq_val_t = optimize_acqf(\n",
    "            ei_on_line,\n",
    "            bounds=one_d_bounds,\n",
    "            q=1,\n",
    "            num_restarts=10,\n",
    "            raw_samples=100\n",
    "        )\n",
    "\n",
    "        alpha_star = cand_t.item()\n",
    "        new_x = self.best_point + alpha_star * direction\n",
    "        new_x_clamped = torch.clamp(new_x, self.bounds[0], self.bounds[1])\n",
    "\n",
    "        return new_x_clamped, alpha_star, acq_val_t.item(), lb, ub"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム1: 元の報酬設計（予測誤差ベース）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:48.087171Z",
     "start_time": "2025-07-11T04:26:48.072074Z"
    }
   },
   "source": [
    "class ECI_BO_Bandit_Continuous_Original(ECI_BO_Bandit_Original):\n",
    "    def __init__(self, X, objective_function, bounds, n_initial, n_max, dim,\n",
    "                 algo_base_name=\"ECI_BO_Bandit_Continuous_Original\", coordinate_ratio=0.8, run_id=1, output_base_dir=\"output_results\"):\n",
    "        super().__init__(X, objective_function, bounds, n_initial, n_max, dim,\n",
    "                         algo_base_name, coordinate_ratio, run_id, output_base_dir)\n",
    "        self.function_name_with_ratio = f\"Continuous_Original_coord_{self.coordinate_ratio:.1f}\"\n",
    "        self.algo_name_for_run = f\"{algo_base_name}_{self.function_name_with_ratio}_run{self.run_id}\"\n",
    "        self.output_dir = os.path.join(output_base_dir, algo_base_name, self.function_name_with_ratio)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def find_optimal_direction_continuous(self):\n",
    "        \"\"\"連続空間での最適方向を見つける\"\"\"\n",
    "        # 1. 現在のLinUCBパラメータを取得\n",
    "        A_t = self.lambda_reg * torch.eye(self.dim, device=self.X.device) + self.A\n",
    "        try:\n",
    "            A_inv = torch.inverse(A_t)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            A_inv = torch.linalg.pinv(A_t)\n",
    "\n",
    "        theta_hat = A_inv @ self.b\n",
    "\n",
    "        # 2. UCBのbetaを計算\n",
    "        current_round_t = self.total_iterations_for_bandit\n",
    "        if current_round_t == 0: current_round_t = 1\n",
    "        log_term_numerator = 1 + (current_round_t - 1) * self.L**2 / self.lambda_reg\n",
    "        if log_term_numerator <= 0: log_term_numerator = 1e-9\n",
    "        beta_t = (self.sigma * math.sqrt(\n",
    "                    self.dim * math.log(log_term_numerator / self.delta))\n",
    "                  + math.sqrt(self.lambda_reg)*self.S)\n",
    "\n",
    "        # 3. 最適なthetaを求める問題を解く\n",
    "        # 二分探索で最適なlambdaを見つける\n",
    "        low = 0.0\n",
    "        high = 1000.0\n",
    "        for _ in range(100):\n",
    "            lam = (low + high) / 2.0\n",
    "            if lam < 1e-9: lam = 1e-9\n",
    "\n",
    "            # theta(lambda)を計算\n",
    "            M = A_t + lam * torch.eye(self.dim, device=self.X.device)\n",
    "            try:\n",
    "                M_inv = torch.inverse(M)\n",
    "            except torch.linalg.LinAlgError:\n",
    "                M_inv = torch.linalg.pinv(M)\n",
    "            \n",
    "            theta_lam = M_inv @ A_t @ theta_hat\n",
    "\n",
    "            # 制約式の評価\n",
    "            diff = theta_lam - theta_hat\n",
    "            val = diff.t() @ A_t @ diff\n",
    "            \n",
    "            if val > beta_t**2:\n",
    "                low = lam\n",
    "            else:\n",
    "                high = lam\n",
    "        \n",
    "        # 最終的なlambdaで optimal theta を計算\n",
    "        lam = (low + high) / 2.0\n",
    "        M = A_t + lam * torch.eye(self.dim, device=self.X.device)\n",
    "        try:\n",
    "            M_inv = torch.inverse(M)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            M_inv = torch.linalg.pinv(M)\n",
    "        \n",
    "        optimal_theta = M_inv @ A_t @ theta_hat\n",
    "\n",
    "        # 4. 方向ベクトルを正規化して返す\n",
    "        norm = torch.norm(optimal_theta)\n",
    "        if norm < 1e-9:\n",
    "            # ノルムがほぼゼロの場合、ランダムな方向を返す\n",
    "            direction = torch.randn(self.dim, device=self.X.device)\n",
    "            return direction / torch.norm(direction)\n",
    "        else:\n",
    "            return optimal_theta / norm\n",
    "\n",
    "    def optimize(self):\n",
    "        self.sigma = 1.0\n",
    "        self.L = 1.0\n",
    "        self.lambda_reg = 1.0\n",
    "        self.delta = 0.1\n",
    "        self.S = 1.0\n",
    "\n",
    "        self.initialize()\n",
    "        n_bo_iter = self.n_initial\n",
    "\n",
    "        while n_bo_iter < self.n_max:\n",
    "            self.total_iterations_for_bandit += 1\n",
    "\n",
    "            # 連続方向探索を使用\n",
    "            direction = self.find_optimal_direction_continuous()\n",
    "            self.selected_direction_history.append(direction.clone())\n",
    "\n",
    "            new_x, _, _, _, _ = self.propose_new_x(direction)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_mean_at_new_x = self.model.posterior(new_x.unsqueeze(0)).mean.squeeze().item()\n",
    "            actual_y_at_new_x = self.objective_function(new_x.unsqueeze(0)).squeeze().item()\n",
    "            \n",
    "            # 元の報酬設計: 予測誤差ベース\n",
    "            prediction_error = abs(predicted_mean_at_new_x - actual_y_at_new_x)\n",
    "            reward = 10.0 * (1.0 - math.exp(-prediction_error / self.scale_init))\n",
    "\n",
    "            x_arm_for_update = direction.view(-1, 1)\n",
    "            self.A += x_arm_for_update @ x_arm_for_update.t()\n",
    "            self.b += reward * direction  # 元の報酬設計\n",
    "\n",
    "            self.X = torch.cat([self.X, new_x.unsqueeze(0)], 0)\n",
    "            self.Y = torch.cat([self.Y, torch.tensor([[actual_y_at_new_x]], dtype=torch.float32, device=self.X.device)], 0)\n",
    "            self.update_model()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                posterior_mean_overall = self.model.posterior(self.X).mean.squeeze(-1)\n",
    "            current_best_idx = posterior_mean_overall.argmin()\n",
    "            self.best_value = posterior_mean_overall[current_best_idx].item()\n",
    "            self.best_point = self.X[current_best_idx]\n",
    "            self.eval_history.append(self.best_value)\n",
    "            n_bo_iter += 1\n",
    "\n",
    "        return self.best_point, self.best_value"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム2: 新しい報酬設計（勾配ベース）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:48.118229Z",
     "start_time": "2025-07-11T04:26:48.103172Z"
    }
   },
   "source": [
    "class ECI_BO_Bandit_Continuous_Gradient(ECI_BO_Bandit_Original):\n",
    "    def __init__(self, X, objective_function, bounds, n_initial, n_max, dim,\n",
    "                 algo_base_name=\"ECI_BO_Bandit_Continuous_Gradient\", coordinate_ratio=0.8, run_id=1, output_base_dir=\"output_results\"):\n",
    "        super().__init__(X, objective_function, bounds, n_initial, n_max, dim,\n",
    "                         algo_base_name, coordinate_ratio, run_id, output_base_dir)\n",
    "        self.function_name_with_ratio = f\"Continuous_Gradient_coord_{self.coordinate_ratio:.1f}\"\n",
    "        self.algo_name_for_run = f\"{algo_base_name}_{self.function_name_with_ratio}_run{self.run_id}\"\n",
    "        self.output_dir = os.path.join(output_base_dir, algo_base_name, self.function_name_with_ratio)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def find_optimal_direction_continuous(self):\n",
    "        \"\"\"連続空間での最適方向を見つける\"\"\"\n",
    "        # 1. 現在のLinUCBパラメータを取得\n",
    "        A_t = self.lambda_reg * torch.eye(self.dim, device=self.X.device) + self.A\n",
    "        try:\n",
    "            A_inv = torch.inverse(A_t)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            A_inv = torch.linalg.pinv(A_t)\n",
    "\n",
    "        theta_hat = A_inv @ self.b\n",
    "\n",
    "        # 2. UCBのbetaを計算\n",
    "        current_round_t = self.total_iterations_for_bandit\n",
    "        if current_round_t == 0: current_round_t = 1\n",
    "        log_term_numerator = 1 + (current_round_t - 1) * self.L**2 / self.lambda_reg\n",
    "        if log_term_numerator <= 0: log_term_numerator = 1e-9\n",
    "        beta_t = (self.sigma * math.sqrt(\n",
    "                    self.dim * math.log(log_term_numerator / self.delta))\n",
    "                  + math.sqrt(self.lambda_reg)*self.S)\n",
    "\n",
    "        # 3. 最適なthetaを求める問題を解く\n",
    "        # 二分探索で最適なlambdaを見つける\n",
    "        low = 0.0\n",
    "        high = 1000.0\n",
    "        for _ in range(100):\n",
    "            lam = (low + high) / 2.0\n",
    "            if lam < 1e-9: lam = 1e-9\n",
    "\n",
    "            # theta(lambda)を計算\n",
    "            M = A_t + lam * torch.eye(self.dim, device=self.X.device)\n",
    "            try:\n",
    "                M_inv = torch.inverse(M)\n",
    "            except torch.linalg.LinAlgError:\n",
    "                M_inv = torch.linalg.pinv(M)\n",
    "            \n",
    "            theta_lam = M_inv @ A_t @ theta_hat\n",
    "\n",
    "            # 制約式の評価\n",
    "            diff = theta_lam - theta_hat\n",
    "            val = diff.t() @ A_t @ diff\n",
    "            \n",
    "            if val > beta_t**2:\n",
    "                low = lam\n",
    "            else:\n",
    "                high = lam\n",
    "        \n",
    "        # 最終的なlambdaで optimal theta を計算\n",
    "        lam = (low + high) / 2.0\n",
    "        M = A_t + lam * torch.eye(self.dim, device=self.X.device)\n",
    "        try:\n",
    "            M_inv = torch.inverse(M)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            M_inv = torch.linalg.pinv(M)\n",
    "        \n",
    "        optimal_theta = M_inv @ A_t @ theta_hat\n",
    "\n",
    "        # 4. 方向ベクトルを正規化して返す\n",
    "        norm = torch.norm(optimal_theta)\n",
    "        if norm < 1e-9:\n",
    "            # ノルムがほぼゼロの場合、ランダムな方向を返す\n",
    "            direction = torch.randn(self.dim, device=self.X.device)\n",
    "            return direction / torch.norm(direction)\n",
    "        else:\n",
    "            return optimal_theta / norm\n",
    "\n",
    "    def optimize(self):\n",
    "        self.sigma = 1.0\n",
    "        self.L = 1.0\n",
    "        self.lambda_reg = 1.0\n",
    "        self.delta = 0.1\n",
    "        self.S = 1.0\n",
    "\n",
    "        self.initialize()\n",
    "        n_bo_iter = self.n_initial\n",
    "\n",
    "        while n_bo_iter < self.n_max:\n",
    "            self.total_iterations_for_bandit += 1\n",
    "\n",
    "            # 連続方向探索を使用\n",
    "            direction = self.find_optimal_direction_continuous()\n",
    "            self.selected_direction_history.append(direction.clone())\n",
    "\n",
    "            new_x, _, _, _, _ = self.propose_new_x(direction)\n",
    "\n",
    "            # 実際の評価\n",
    "            actual_y_at_new_x = self.objective_function(new_x.unsqueeze(0)).squeeze().item()\n",
    "            \n",
    "            # --- 新しい報酬計算 ---\n",
    "            new_x_for_grad = new_x.clone().unsqueeze(0)\n",
    "            new_x_for_grad.requires_grad_(True)\n",
    "\n",
    "            # GPモデルで事後分布を取得\n",
    "            posterior = self.model.posterior(new_x_for_grad)\n",
    "            mean_at_new_x = posterior.mean\n",
    "\n",
    "            # 勾配を計算\n",
    "            mean_at_new_x.sum().backward()\n",
    "            grad_vector = new_x_for_grad.grad.squeeze(0)\n",
    "\n",
    "            # 報酬ベクトルを定義 (絶対値を取ることで影響の大きさを評価)\n",
    "            reward_vector = grad_vector.abs() \n",
    "            \n",
    "            # banditパラメータを更新\n",
    "            x_arm_for_update = direction.view(-1, 1)\n",
    "            self.A += x_arm_for_update @ x_arm_for_update.t()\n",
    "            self.b += reward_vector  # 新しい報酬設計\n",
    "            # --- 新しい報酬計算ここまで ---\n",
    "\n",
    "            self.X = torch.cat([self.X, new_x.unsqueeze(0)], 0)\n",
    "            self.Y = torch.cat([self.Y, torch.tensor([[actual_y_at_new_x]], dtype=torch.float32, device=self.X.device)], 0)\n",
    "            self.update_model()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                posterior_mean_overall = self.model.posterior(self.X).mean.squeeze(-1)\n",
    "            current_best_idx = posterior_mean_overall.argmin()\n",
    "            self.best_value = posterior_mean_overall[current_best_idx].item()\n",
    "            self.best_point = self.X[current_best_idx]\n",
    "            self.eval_history.append(self.best_value)\n",
    "            n_bo_iter += 1\n",
    "\n",
    "        return self.best_point, self.best_value"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験実行と比較"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T04:26:48.149230Z",
     "start_time": "2025-07-11T04:26:48.135228Z"
    }
   },
   "source": [
    "def generate_initial_points(n_initial, dim, bounds):\n",
    "    return torch.rand(n_initial, dim) * (bounds[1] - bounds[0]) + bounds[0]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T08:05:48.393960Z",
     "start_time": "2025-07-11T04:26:48.165739Z"
    }
   },
   "source": "if __name__ == \"__main__\":\n    test_funcs = [\n        (\"StyblinskiTang\", styblinski_tang_100d, -195.83),\n        (\"Rastrigin\", rastrigin_100d, 0.0),\n        (\"Ackley\", ackley_100d, 0.0),\n    ]\n    dim = 20\n    bounds = torch.tensor([[-5.0]*dim, [5.0]*dim], dtype=torch.float32)\n    n_initial = 5\n    n_iter = 300  \n    n_runs = 20\n\n    output_base_dir = \"output_results_reward_comparison\"\n    os.makedirs(output_base_dir, exist_ok=True)\n\n    coordinate_ratio = 0.8\n\n    # 全実行で共通の初期点\n    initial_points_all_runs = [\n        generate_initial_points(n_initial, dim, bounds)\n        for _ in range(n_runs)\n    ]\n\n    algorithms = [\n        (\"Original_Reward\", ECI_BO_Bandit_Continuous_Original),\n        (\"Gradient_Reward\", ECI_BO_Bandit_Continuous_Gradient)\n    ]\n\n    for func_name_short, func_eval, global_opt_val in test_funcs:\n        print(f\"========== テスト関数実行中: {func_name_short} ==========\")\n\n        # 全アルゴリズムの結果を保存\n        all_algorithm_results = {}\n\n        for algo_name, algo_class in algorithms:\n            print(f\"--- {algo_name} アルゴリズム実行中 ---\")\n            \n            histories_for_this_algo = []\n            dim_sums_for_this_algo = []\n\n            # tqdmの代わりにシンプルなプログレス表示を使用\n            for run_idx in range(n_runs):\n                print(f\"\\r  実行中: {run_idx + 1}/{n_runs}\", end=\"\", flush=True)\n                \n                initial_X_for_run = initial_points_all_runs[run_idx].clone().to(dtype=torch.float32)\n\n                optimizer = algo_class(\n                    X=initial_X_for_run,\n                    objective_function=func_eval,\n                    bounds=bounds,\n                    n_initial=n_initial,\n                    n_max=n_iter,\n                    dim=dim,\n                    algo_base_name=func_name_short,\n                    coordinate_ratio=coordinate_ratio,\n                    run_id=run_idx + 1,\n                    output_base_dir=output_base_dir\n                )\n\n                _, _ = optimizer.optimize()\n\n                histories_for_this_algo.append(optimizer.eval_history)\n\n                if optimizer.selected_direction_history:\n                    directions_tensor = torch.stack(optimizer.selected_direction_history, 0)\n                    abs_sum_per_dim = directions_tensor.abs().sum(dim=0).cpu().numpy()\n                    dim_sums_for_this_algo.append(abs_sum_per_dim)\n                else:\n                    dim_sums_for_this_algo.append(np.zeros(dim))\n            \n            print()  # 改行\n\n            # 収束統計の計算\n            eval_histories_np_array = np.array(histories_for_this_algo)\n            mean_convergence = eval_histories_np_array.mean(axis=0)\n            std_convergence = eval_histories_np_array.std(axis=0)\n\n            if dim_sums_for_this_algo:\n                avg_dim_abs_sum = np.mean(np.stack(dim_sums_for_this_algo, 0), axis=0)\n            else:\n                avg_dim_abs_sum = np.zeros(dim)\n\n            all_algorithm_results[algo_name] = {\n                'mean_hist': mean_convergence,\n                'std_hist': std_convergence,\n                'avg_dim_abs_sum': avg_dim_abs_sum\n            }\n\n            # 個別アルゴリズムの方向プロットを保存\n            plot_save_dir = os.path.join(output_base_dir, func_name_short, f\"{algo_name}_coord_{coordinate_ratio:.1f}\")\n            os.makedirs(plot_save_dir, exist_ok=True)\n            \n            plt.figure(figsize=(10, 6))\n            plt.bar(np.arange(dim), avg_dim_abs_sum, alpha=0.7)\n            plt.xlabel(\"次元インデックス\", fontsize=12)\n            plt.ylabel(\"方向成分絶対値の平均和\", fontsize=12)\n            title_str = (f\"{func_name_short} - {algo_name} (coord_ratio={coordinate_ratio:.1f})\\n\"\n                        f\"{n_runs}回実行での方向絶対値平均和\")\n            plt.title(title_str, fontsize=14)\n            plt.xticks(np.arange(0, dim, step=max(1, dim//10)), fontsize=10)\n            plt.yticks(fontsize=10)\n            plt.grid(axis='y', linestyle='--', alpha=0.7)\n            plt.tight_layout()\n            plt.savefig(os.path.join(plot_save_dir, \"average_dimension_abs_sum.png\"), dpi=150)\n            plt.close()\n\n        # 比較収束プロットの作成\n        plt.figure(figsize=(12, 8))\n        iters_plot = np.arange(1, n_iter + 1)\n        \n        colors = ['blue', 'red']\n        for i, (algo_name, results) in enumerate(all_algorithm_results.items()):\n            plt.plot(iters_plot, results['mean_hist'], \n                    label=f\"{algo_name}\", color=colors[i], linewidth=2)\n            plt.fill_between(iters_plot,\n                           results['mean_hist'] - results['std_hist'],\n                           results['mean_hist'] + results['std_hist'],\n                           alpha=0.2, color=colors[i])\n\n        plt.axhline(global_opt_val, color='green', linestyle='--', label='大域最適値', linewidth=2)\n        plt.xlabel(\"評価回数\", fontsize=14)\n        plt.ylabel(\"発見された最良目的値 (平均 ± 標準偏差)\", fontsize=14)\n        plt.title(f\"{func_name_short}での報酬設計比較\\n(coordinate_ratio={coordinate_ratio:.1f})\", fontsize=16)\n        plt.legend(fontsize=12)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.tight_layout()\n\n        # 比較プロットの保存\n        comparison_plot_save_dir = os.path.join(output_base_dir, func_name_short)\n        os.makedirs(comparison_plot_save_dir, exist_ok=True)\n        plt.savefig(os.path.join(comparison_plot_save_dir, f\"{func_name_short}_reward_comparison.png\"), dpi=150)\n        plt.close()\n\n        # 方向比較プロットの作成\n        plt.figure(figsize=(12, 5))\n        for i, (algo_name, results) in enumerate(all_algorithm_results.items()):\n            plt.subplot(1, 2, i+1)\n            plt.bar(np.arange(dim), results['avg_dim_abs_sum'], alpha=0.7, color=colors[i])\n            plt.xlabel(\"次元インデックス\", fontsize=10)\n            plt.ylabel(\"方向成分絶対値の平均和\", fontsize=10)\n            plt.title(f\"{algo_name}\\n{func_name_short}\", fontsize=12)\n            plt.xticks(np.arange(0, dim, step=max(1, dim//5)), fontsize=8)\n            plt.yticks(fontsize=8)\n            plt.grid(axis='y', linestyle='--', alpha=0.5)\n            \n        plt.suptitle(f\"方向使用比較 - {func_name_short}\", fontsize=14)\n        plt.tight_layout()\n        plt.savefig(os.path.join(comparison_plot_save_dir, f\"{func_name_short}_direction_comparison.png\"), dpi=150)\n        plt.close()\n\n        print(f\"========== テスト関数完了: {func_name_short} ==========\")\n\n    print(\"全ての実験が完了しました。\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== テスト関数実行中: StyblinskiTang ==========\n",
      "--- Original_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "--- Gradient_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "========== テスト関数完了: StyblinskiTang ==========\n",
      "========== テスト関数実行中: Rastrigin ==========\n",
      "--- Original_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "--- Gradient_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "========== テスト関数完了: Rastrigin ==========\n",
      "========== テスト関数実行中: Ackley ==========\n",
      "--- Original_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "--- Gradient_Reward アルゴリズム実行中 ---\n",
      "  実行中: 20/20\n",
      "========== テスト関数完了: Ackley ==========\n",
      "全ての実験が完了しました。\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の分析\n",
    "\n",
    "実験完了後、以下の分析を行います：\n",
    "\n",
    "1. **収束性能の比較**\n",
    "   - 元の報酬設計 vs 新しい報酬設計の収束速度\n",
    "   - 最終的な最適化性能の差\n",
    "\n",
    "2. **方向選択の比較**\n",
    "   - 各次元の使用頻度の違い\n",
    "   - 有効次元（0-4）への集中度\n",
    "\n",
    "3. **報酬設計の影響**\n",
    "   - 予測誤差ベース vs 勾配ベースの特徴\n",
    "   - 各テスト関数での効果の違い\n",
    "\n",
    "実験を実行して結果を確認してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
